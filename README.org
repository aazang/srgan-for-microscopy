#+TITLE: SRGAN for microscopy
#+AUTHOR: Mario Baars

* About
This repository was created to host the codebase of my bachelors thesis. The topic of it was (translated to english):
Resolution enhancement of image data from optical microscopy. I analysed the results of single-image-super-resolution
(SISR) with the Super-Resoltion-Generative-Adversarial-Network (SRGAN), while changing the input parameters of the neural
net. I analysed the resulting super resolved images based on realism, as well focused on the needed resources
for the training and test loops.

* The Dataset
The dataset which was used for this thesis was the [[https://www.sciencedirect.com/science/article/pii/S2352340920303681][PBC dataset]]. It contains 17092 images of blood cells, categorized into
eight diffrent groups.

* Code basis
The basis for the code in this project, was the [[https://www.kaggle.com/balraj98/single-image-super-resolution-gan-srgan-pytorch/notebook][code implementation]] of the user Balraj Ashwath on kaggle.com, in which he
implemented the SRGAN architecture which was introduced by [[https://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf][Christian Ledig et. al.]].

Modifications to the original code base included:
- Refactorization into 3 files:
  - =main/models.py= which contains the model definitions of the Generator, Discriminator and the vgg19-network, which
    contributs to the loss function.
  - =main/dataset_class.py= which contains the dataset class, which is used to load the training and test images.
  - =training_modified.py= which calls on the other two files to train the network.
- Adding data collection features

* Hardware
One big aspect of this bachelor thesis was the performance under limited resources. Large image datasets need a lot of
hardware resources to effectively train a neural network. The GPU used to train the SRGAN in this thesis was a NVIDIA RTX
2080 Ti, which is, from a consumer perspective a high end card, but unfortunately not best suited for training neural
networks, due to its limited VRAM size of 11 GB.

Due to the basic nature of this thesis, more advanced techniques to reduce the hardware resources required where not used.
The focus was soley on changing the basic input parameters and analysing the results.

* Premise
The input parameters, that were variied were:
- epoch count
- batch size
- image dimensions

Another aspect, which this thesis evaluted is the result of various manipulations of the dataset:
- size of dataset
- zoomed in versions of the images, for a bigger emphasize on the central blood cell
- added noise, variing in intensity
- adding different images to the original dataset

* Results
The quality measure was the value of the loss functions. They are closely correlated to image quality. The optimal
value for a perfectly trained Generator would be 0 and the for the Discriminator 0.5. This would mean, that
the Discriminator has to guess if the image, it recieved was real or generated.
The loss function value of 0 from the Generator would mean, that it can perfectly mimic the real data distribution and can
draw examples from it.

The parameter ~batchsize~ is the number of images, that are simultaneously used as input within one training iteration.
Loss functions, needed VRAM, as well as the time used for training with respect to batch size:
#+caption: Performance metrics for different batchsizes. 
#+attr_org: :width 800
#+attr_html: :width 800
[[./plots/Batchsizes_PBC_info.png]]
Performance metrics for different batchsizes.

Example SISR of one image:
#+caption: On the left side is the original, in the middle the, by the factor 4 downscaled version and on the right is the superresolved image from the SRGAN.
#+attr_org: :width 800
#+attr_html: :width 800
[[./example_images/10.png]]
On the left side is the original, in the middle downscaled version and on the right is the super
resolved image from the SRGAN.
